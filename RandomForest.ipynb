{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZ+6esg6tCtbghCh1Sso+t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mlacasa/EstadisticaQ2/blob/main/RandomForest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#üöÄ Ejercicio: Evaluaci√≥n de un Modelo Random Forest\n",
        "¬°Bienvenido/a a este ejercicio pr√°ctico! El objetivo es construir y evaluar un modelo de clasificaci√≥n utilizando el algoritmo Random Forest. Para ello, trabajaremos con el conocido dataset \"Pima Indians Diabetes\", que contiene informaci√≥n diagn√≥stica de pacientes para predecir la aparici√≥n de diabetes.\n",
        "\n",
        "A lo largo de este cuaderno, seguiremos los pasos clave para entrenar y validar un modelo de machine learning de forma rigurosa.\n",
        "\n",
        "##Paso 1: Carga y Preparaci√≥n de los Datos\n",
        "Toda gran aventura en machine learning comienza con los datos. En esta primera celda, realizaremos las tareas fundamentales de preparaci√≥n para dejar nuestro dataset listo para el entrenamiento.\n",
        "\n",
        "Los pasos que ejecutar√° el siguiente c√≥digo son:\n",
        "\n",
        "Importar Librer√≠as: Cargaremos las herramientas que necesitaremos, como pandas para manejar los datos y scikit-learn para el preprocesamiento y el modelo.\n",
        "\n",
        "Cargar el Dataset: Accederemos a los datos desde una URL p√∫blica y los cargaremos en un DataFrame de pandas, asignando nombres descriptivos a cada columna.\n",
        "\n",
        "Separar Caracter√≠sticas y Objetivo: Dividiremos el dataset en dos partes:\n",
        "\n",
        "X: Las variables predictoras (ej. 'Glucosa', 'IMC', 'Edad').\n",
        "\n",
        "y: La variable objetivo que queremos predecir ('Outcome', si la paciente tiene o no diabetes).\n",
        "\n",
        "Dividir en Entrenamiento y Prueba: Separaremos nuestros datos en un conjunto de entrenamiento (80%) y uno de prueba (20%). Esto es crucial para evaluar de forma honesta el rendimiento del modelo en datos que no ha visto previamente.\n",
        "\n",
        "Escalar las Caracter√≠sticas: Aunque Random Forest no es tan sensible a la escala de las variables como los modelos lineales, es una buena pr√°ctica estandarizar los datos. Usaremos StandardScaler para asegurar que todas las caracter√≠sticas tengan una media de 0 y una desviaci√≥n est√°ndar de 1.\n",
        "\n",
        "¬°Empecemos!"
      ],
      "metadata": {
        "id": "eIni8TtlhZy5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLatMXutjEGJ"
      },
      "outputs": [],
      "source": [
        "# --- 1. IMPORTACI√ìN DE LIBRER√çAS ---\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# --- 2. CARGA DEL DATASET ---\n",
        "# Se accede al dataset desde una URL p√∫blica del repositorio de UCI\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "column_names = ['Embarazos', 'Glucosa', 'Presion', 'Piel', 'Insulina', 'IMC', 'Pedigri', 'Edad', 'Outcome']\n",
        "data = pd.read_csv(url, names=column_names)\n",
        "\n",
        "print(\"--- Primeras 5 filas del dataset ---\")\n",
        "print(data.head())\n",
        "\n",
        "# --- 3. SEPARACI√ìN DE CARACTER√çSTICAS (X) Y OBJETIVO (y) ---\n",
        "X = data.drop('Outcome', axis=1)\n",
        "y = data['Outcome']\n",
        "\n",
        "# --- 4. DIVISI√ìN EN CONJUNTOS DE ENTRENAMIENTO Y PRUEBA ---\n",
        "# Se utiliza stratify=y para mantener la proporci√≥n de 0s y 1s en ambos conjuntos\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"\\nN√∫mero de muestras de entrenamiento: {X_train.shape[0]}\")\n",
        "print(f\"N√∫mero de muestras de prueba: {X_test.shape[0]}\")\n",
        "\n",
        "# --- 5. ESCALADO DE CARACTER√çSTICAS ---\n",
        "# Se crea el objeto escalador\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Se ajusta el escalador S√ìLO con los datos de entrenamiento para evitar fuga de informaci√≥n\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Se aplica la misma transformaci√≥n a los datos de prueba\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"\\n¬°Datos listos para el entrenamiento del modelo!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Paso 2: Entrenamiento y Evaluaci√≥n del Modelo Random Forest\n",
        "Ahora que nuestros datos est√°n listos, vamos a entrenar nuestro modelo Random Forest. Este algoritmo es un \"ensamblaje\", lo que significa que construye m√∫ltiples √°rboles de decisi√≥n y combina sus predicciones para obtener un resultado m√°s preciso y robusto.\n",
        "\n",
        "En esta celda, realizaremos tres an√°lisis clave para entender nuestro modelo:\n",
        "\n",
        "Evaluaci√≥n de Rendimiento: Crearemos una matriz de confusi√≥n detallada y un reporte de clasificaci√≥n para medir qu√© tan bien funciona nuestro \"bosque\" en la tarea de predicci√≥n.\n",
        "\n",
        "An√°lisis de Importancia de Variables: Una de las grandes ventajas de Random Forest es que puede decirnos qu√© variables fueron m√°s importantes para tomar sus decisiones. Lo visualizaremos con un gr√°fico de barras.\n",
        "\n",
        "Visualizaci√≥n de un √Årbol Individual: Para entender qu√© hay dentro del \"bosque\", aislaremos y dibujaremos uno de los √°rboles de decisi√≥n que lo componen. Esto nos dar√° una idea de las reglas que el modelo aprende."
      ],
      "metadata": {
        "id": "HiOZCYVAh6v8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se asume que las variables X_train_scaled, X_test_scaled, y_train, y_test est√°n disponibles.\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. ENTRENAMIENTO DEL MODELO RANDOM FOREST ---\n",
        "# Instanciamos el modelo con random_state para que los resultados sean reproducibles\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "\n",
        "print(\"--- Entrenando el modelo Random Forest ---\")\n",
        "rf_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Realizar predicciones en el conjunto de prueba\n",
        "y_pred_rf = rf_model.predict(X_test_scaled)\n",
        "\n",
        "\n",
        "# --- 2. EVALUACI√ìN DE RENDIMIENTO ---\n",
        "print(\"\\n--- Matriz de Confusi√≥n del Modelo Random Forest ---\")\n",
        "cm = confusion_matrix(y_test, y_pred_rf)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "accuracy = accuracy_score(y_test, y_pred_rf)\n",
        "\n",
        "# Crear etiquetas detalladas para la matriz de confusi√≥n\n",
        "labels = np.array([\n",
        "    f\"Verdadero Negativo\\n\\n{tn}\", f\"Falso Positivo\\n\\n{fp}\",\n",
        "    f\"Falso Negativo\\n\\n{fn}\", f\"Verdadero Positivo\\n\\n{tp}\"\n",
        "]).reshape(2, 2)\n",
        "\n",
        "# Visualizar la matriz\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=labels, fmt='', cmap='Blues', cbar=False, annot_kws={\"size\": 14})\n",
        "plt.title(f'Matriz de Confusi√≥n (Exactitud Global: {accuracy:.2%})', fontsize=16)\n",
        "plt.ylabel('Etiqueta Real', fontsize=12)\n",
        "plt.xlabel('Etiqueta Predicha', fontsize=12)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n--- Reporte de Clasificaci√≥n ---\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "\n",
        "\n",
        "# --- 3. AN√ÅLISIS DE IMPORTANCIA DE VARIABLES ---\n",
        "# Extraer la importancia de cada caracter√≠stica\n",
        "importances = rf_model.feature_importances_\n",
        "feature_names = X.columns\n",
        "\n",
        "# Crear un DataFrame para la visualizaci√≥n\n",
        "importance_df = pd.DataFrame({'Variable': feature_names, 'Importancia': importances})\n",
        "importance_df = importance_df.sort_values(by='Importancia', ascending=False)\n",
        "\n",
        "# Graficar la importancia de las variables\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x='Importancia', y='Variable', data=importance_df, palette='viridis')\n",
        "plt.title('Importancia de las Variables seg√∫n Random Forest', fontsize=16)\n",
        "plt.xlabel('Importancia Relativa', fontsize=12)\n",
        "plt.ylabel('Variable', fontsize=12)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# --- 4. VISUALIZACI√ìN DE UN √ÅRBOL INDIVIDUAL DEL BOSQUE ---\n",
        "# Vamos a visualizar el primer √°rbol del bosque (√≠ndice 0)\n",
        "single_tree = rf_model.estimators_[0]\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "# Dibujar el √°rbol con una profundidad m√°xima de 3 para que sea legible\n",
        "plot_tree(single_tree,\n",
        "          feature_names=X.columns,\n",
        "          class_names=['Sano', 'Diab√©tico'],\n",
        "          filled=True,\n",
        "          rounded=True,\n",
        "          max_depth=3,\n",
        "          fontsize=10)\n",
        "plt.title('Visualizaci√≥n de un √Årbol de Decisi√≥n Individual del Bosque', fontsize=18)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "90ibrJJhk3qP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Paso 3: Optimizaci√≥n con GridSearchCV y Comparaci√≥n de Resultados\n",
        "Aunque nuestro modelo Random Forest inicial tuvo un buen rendimiento, casi siempre es posible mejorarlo mediante la optimizaci√≥n de hiperpar√°metros. En lugar de usar los valores por defecto del modelo, vamos a buscar la combinaci√≥n de par√°metros que ofrezca el mejor resultado para nuestro dataset espec√≠fico.\n",
        "\n",
        "¬øC√≥mo lo haremos?\n",
        "\n",
        "Definir una \"Parrilla\" de B√∫squeda: Crearemos un diccionario (param_grid) con los hiperpar√°metros clave de Random Forest que queremos probar (como el n√∫mero de √°rboles, la profundidad m√°xima, etc.) y una lista de posibles valores para cada uno.\n",
        "\n",
        "Usar GridSearchCV: Esta potente herramienta de scikit-learn probar√° sistem√°ticamente todas las combinaciones posibles de la parrilla utilizando validaci√≥n cruzada. Esto asegura que encontremos la mejor combinaci√≥n de forma robusta.\n",
        "\n",
        "Comparar Antes y Despu√©s: Una vez que GridSearchCV encuentre el mejor modelo, generaremos un gr√°fico comparativo para ver si hemos logrado una mejora en 5 m√©tricas clave (Accuracy, Precision, Recall,"
      ],
      "metadata": {
        "id": "S7G5td4piZtc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se asume que las variables X_train_scaled, X_test_scaled, y_train, y_test\n",
        "# y el modelo 'rf_model' por defecto est√°n disponibles.\n",
        "\n",
        "# --- 0. LIBRER√çAS Y FUNCIONES DE AYUDA ---\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.stats import randint, uniform\n",
        "\n",
        "# Librer√≠as para la barra de progreso\n",
        "from tqdm.notebook import tqdm\n",
        "import joblib\n",
        "import contextlib\n",
        "\n",
        "# --- PASO PREVIO: EVALUACI√ìN DEL MODELO BASE ---\n",
        "print(\"--- Evaluaci√≥n del Modelo Base ---\")\n",
        "y_pred_base = rf_model.predict(X_test_scaled)\n",
        "y_prob_base = rf_model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "base_metrics = {\n",
        "    'Accuracy': accuracy_score(y_test, y_pred_base),\n",
        "    'Precision': precision_score(y_test, y_pred_base),\n",
        "    'Recall': recall_score(y_test, y_pred_base),\n",
        "    'F1 Score': f1_score(y_test, y_pred_base),\n",
        "    'AUC': roc_auc_score(y_test, y_prob_base)\n",
        "}\n",
        "\n",
        "print(\"M√©tricas del modelo base:\")\n",
        "for metric, value in base_metrics.items():\n",
        "    print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "target_auc = base_metrics['AUC']\n",
        "print(f\"\\nObjetivo: Superar AUC de {target_auc:.4f}\")\n",
        "\n",
        "# --- 1. DEFINICI√ìN DE M√öLTIPLES ESTRATEGIAS DE B√öSQUEDA ---\n",
        "print(\"\\n--- Paso 1: Definiendo Estrategias de B√∫squeda ---\")\n",
        "\n",
        "# Estrategia 1: Grid b√°sico (m√°s conservador)\n",
        "param_grid_basic = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [10, 15, 20, None],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2],\n",
        "    'max_features': ['sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Estrategia 2: Grid amplio (m√°s exploraci√≥n)\n",
        "param_grid_extended = {\n",
        "    'n_estimators': [50, 100, 200, 300, 500],\n",
        "    'max_depth': [5, 10, 15, 20, 25, None],\n",
        "    'min_samples_split': [2, 5, 10, 20],\n",
        "    'min_samples_leaf': [1, 2, 4, 8],\n",
        "    'max_features': ['sqrt', 'log2', 0.3, 0.5, 0.7],\n",
        "    'bootstrap': [True, False],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "# Estrategia 3: Randomized Search (para espacios muy grandes)\n",
        "param_dist_random = {\n",
        "    'n_estimators': randint(50, 500),\n",
        "    'max_depth': [5, 10, 15, 20, 25, 30, None],\n",
        "    'min_samples_split': randint(2, 25),\n",
        "    'min_samples_leaf': randint(1, 10),\n",
        "    'max_features': uniform(0.1, 0.9),\n",
        "    'bootstrap': [True, False],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "# --- 2. FUNCI√ìN PARA EJECUTAR B√öSQUEDA CON BARRA DE PROGRESO ---\n",
        "@contextlib.contextmanager\n",
        "def tqdm_joblib(tqdm_object):\n",
        "    \"\"\"Context manager para que joblib (motor de GridSearchCV) informe a tqdm.\"\"\"\n",
        "    class TqdmBatchCompletionCallback(joblib.parallel.BatchCompletionCallBack):\n",
        "        def __call__(self, *args, **kwargs):\n",
        "            tqdm_object.update(n=self.batch_size)\n",
        "            return super().__call__(*args, **kwargs)\n",
        "\n",
        "    old_batch_callback = joblib.parallel.BatchCompletionCallBack\n",
        "    joblib.parallel.BatchCompletionCallBack = TqdmBatchCompletionCallback\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        joblib.parallel.BatchCompletionCallBack = old_batch_callback\n",
        "        tqdm_object.close()\n",
        "\n",
        "def execute_search(search_object, description, total_fits):\n",
        "    \"\"\"Ejecuta la b√∫squeda con barra de progreso\"\"\"\n",
        "    print(f\"\\n{description}\")\n",
        "    print(f\"Se realizar√°n {total_fits} ajustes del modelo.\")\n",
        "\n",
        "    with tqdm_joblib(tqdm(desc=description, total=total_fits)):\n",
        "        search_object.fit(X_train_scaled, y_train)\n",
        "\n",
        "    return search_object\n",
        "\n",
        "# --- 3. ESTRATEGIA ADAPTATIVA DE B√öSQUEDA ---\n",
        "print(\"\\n--- Paso 2: Ejecutando B√∫squeda Adaptativa ---\")\n",
        "\n",
        "best_search = None\n",
        "best_auc = 0\n",
        "strategies_tried = []\n",
        "\n",
        "# Estrategia 1: Grid b√°sico\n",
        "try:\n",
        "    print(\"\\n=== ESTRATEGIA 1: Grid B√°sico ===\")\n",
        "    n_candidates_basic = np.prod([len(v) for v in param_grid_basic.values()])\n",
        "    total_fits_basic = n_candidates_basic * 3  # cv=3\n",
        "\n",
        "    grid_search_basic = GridSearchCV(\n",
        "        estimator=RandomForestClassifier(random_state=42, n_jobs=-1),\n",
        "        param_grid=param_grid_basic,\n",
        "        scoring='roc_auc',\n",
        "        cv=3,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    grid_search_basic = execute_search(grid_search_basic, \"Grid B√°sico\", total_fits_basic)\n",
        "    basic_auc = grid_search_basic.best_score_\n",
        "\n",
        "    strategies_tried.append({\n",
        "        'strategy': 'Grid B√°sico',\n",
        "        'best_auc': basic_auc,\n",
        "        'best_params': grid_search_basic.best_params_,\n",
        "        'search_object': grid_search_basic\n",
        "    })\n",
        "\n",
        "    print(f\"Mejor AUC con Grid B√°sico: {basic_auc:.4f}\")\n",
        "\n",
        "    if basic_auc > best_auc:\n",
        "        best_auc = basic_auc\n",
        "        best_search = grid_search_basic\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error en Grid B√°sico: {e}\")\n",
        "\n",
        "# Si el grid b√°sico no mejora suficiente, probar estrategia extendida\n",
        "if best_auc < target_auc + 0.01:  # Margen de mejora m√≠nimo\n",
        "    try:\n",
        "        print(\"\\n=== ESTRATEGIA 2: Grid Extendido ===\")\n",
        "        print(\"El grid b√°sico no proporcion√≥ mejora suficiente. Probando grid extendido...\")\n",
        "\n",
        "        n_candidates_ext = min(np.prod([len(v) for v in param_grid_extended.values()]), 500)  # Limitar a 500 combinaciones\n",
        "        total_fits_ext = n_candidates_ext * 3\n",
        "\n",
        "        # Si hay demasiadas combinaciones, usar RandomizedSearchCV\n",
        "        if n_candidates_ext > 200:\n",
        "            print(\"Demasiadas combinaciones. Usando RandomizedSearchCV...\")\n",
        "            grid_search_extended = RandomizedSearchCV(\n",
        "                estimator=RandomForestClassifier(random_state=42, n_jobs=-1),\n",
        "                param_distributions=param_dist_random,\n",
        "                n_iter=150,  # N√∫mero de iteraciones\n",
        "                scoring='roc_auc',\n",
        "                cv=3,\n",
        "                verbose=0,\n",
        "                random_state=42\n",
        "            )\n",
        "            total_fits_ext = 150 * 3\n",
        "        else:\n",
        "            grid_search_extended = GridSearchCV(\n",
        "                estimator=RandomForestClassifier(random_state=42, n_jobs=-1),\n",
        "                param_grid=param_grid_extended,\n",
        "                scoring='roc_auc',\n",
        "                cv=3,\n",
        "                verbose=0\n",
        "            )\n",
        "\n",
        "        grid_search_extended = execute_search(grid_search_extended, \"Grid Extendido/Randomized\", total_fits_ext)\n",
        "        extended_auc = grid_search_extended.best_score_\n",
        "\n",
        "        strategies_tried.append({\n",
        "            'strategy': 'Grid Extendido/Randomized',\n",
        "            'best_auc': extended_auc,\n",
        "            'best_params': grid_search_extended.best_params_,\n",
        "            'search_object': grid_search_extended\n",
        "        })\n",
        "\n",
        "        print(f\"Mejor AUC con Grid Extendido: {extended_auc:.4f}\")\n",
        "\n",
        "        if extended_auc > best_auc:\n",
        "            best_auc = extended_auc\n",
        "            best_search = grid_search_extended\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error en Grid Extendido: {e}\")\n",
        "\n",
        "# --- 4. VALIDACI√ìN DE MEJORA ---\n",
        "print(f\"\\n--- Validaci√≥n de Resultados ---\")\n",
        "print(f\"AUC modelo base: {target_auc:.4f}\")\n",
        "print(f\"Mejor AUC encontrado: {best_auc:.4f}\")\n",
        "\n",
        "if best_search is None:\n",
        "    print(\"‚ùå ERROR: No se pudo ejecutar ninguna estrategia de b√∫squeda.\")\n",
        "    best_search = GridSearchCV(\n",
        "        estimator=RandomForestClassifier(random_state=42, n_jobs=-1),\n",
        "        param_grid={'n_estimators': [200], 'max_depth': [15]},\n",
        "        scoring='roc_auc',\n",
        "        cv=3\n",
        "    )\n",
        "    best_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "elif best_auc <= target_auc:\n",
        "    print(\"‚ö†Ô∏è  ADVERTENCIA: No se logr√≥ mejorar significativamente el modelo base.\")\n",
        "    print(\"Recomendaciones:\")\n",
        "    print(\"1. Revisar la calidad y preprocesamiento de los datos\")\n",
        "    print(\"2. Considerar feature engineering adicional\")\n",
        "    print(\"3. Probar otros algoritmos (XGBoost, LightGBM)\")\n",
        "    print(\"4. Ajustar la m√©trica de evaluaci√≥n si es necesario\")\n",
        "else:\n",
        "    improvement = ((best_auc - target_auc) / target_auc) * 100\n",
        "    print(f\"‚úÖ √âXITO: Mejora del {improvement:.2f}% en AUC\")\n",
        "\n",
        "# --- 5. RESUMEN DE ESTRATEGIAS PROBADAS ---\n",
        "print(f\"\\n--- Resumen de Estrategias ---\")\n",
        "strategies_df = pd.DataFrame(strategies_tried)\n",
        "if not strategies_df.empty:\n",
        "    strategies_df = strategies_df.sort_values('best_auc', ascending=False)\n",
        "    print(strategies_df[['strategy', 'best_auc']].to_string(index=False))\n",
        "\n",
        "# --- 6. AN√ÅLISIS DE LA EVOLUCI√ìN DE LA B√öSQUEDA ---\n",
        "print(\"\\n--- An√°lisis de la Evoluci√≥n de Resultados ---\")\n",
        "cv_results_df = pd.DataFrame(best_search.cv_results_)\n",
        "\n",
        "# Seleccionar columnas relevantes din√°micamente\n",
        "param_cols = [col for col in cv_results_df.columns if col.startswith('param_')]\n",
        "relevant_cols = param_cols + ['mean_test_score', 'mean_fit_time']\n",
        "available_cols = [col for col in relevant_cols if col in cv_results_df.columns]\n",
        "\n",
        "cv_summary_df = cv_results_df[available_cols].copy()\n",
        "cv_summary_df = cv_summary_df.rename(columns={'mean_test_score': 'AUC_Medio', 'mean_fit_time': 'Tiempo_Ajuste_s'})\n",
        "cv_summary_df = cv_summary_df.fillna('None')\n",
        "\n",
        "print(\"Mejores 10 combinaciones de hiperpar√°metros encontradas:\")\n",
        "print(cv_summary_df.sort_values(by='AUC_Medio', ascending=False).head(10))\n",
        "\n",
        "# Gr√°fico de evoluci√≥n (adaptativo seg√∫n par√°metros disponibles)\n",
        "if 'param_n_estimators' in cv_summary_df.columns:\n",
        "    plt.figure(figsize=(12, 7))\n",
        "\n",
        "    # Usar max_depth si est√° disponible, sino usar otro par√°metro\n",
        "    size_param = None\n",
        "    hue_param = None\n",
        "\n",
        "    if 'param_max_depth' in cv_summary_df.columns:\n",
        "        size_param = 'param_max_depth'\n",
        "        hue_param = 'param_max_depth'\n",
        "    elif 'param_min_samples_split' in cv_summary_df.columns:\n",
        "        size_param = 'param_min_samples_split'\n",
        "        hue_param = 'param_min_samples_split'\n",
        "\n",
        "    if size_param and hue_param:\n",
        "        sns.scatterplot(data=cv_summary_df, x='param_n_estimators', y='AUC_Medio',\n",
        "                       size=size_param, hue=hue_param, palette='viridis',\n",
        "                       sizes=(50, 250), alpha=0.7)\n",
        "        plt.legend(title=size_param.replace('param_', ''))\n",
        "    else:\n",
        "        sns.scatterplot(data=cv_summary_df, x='param_n_estimators', y='AUC_Medio', alpha=0.7)\n",
        "\n",
        "    plt.title('Evoluci√≥n del Rendimiento (AUC) vs. Hiperpar√°metros', fontsize=16)\n",
        "    plt.xlabel('N√∫mero de √Årboles (n_estimators)', fontsize=12)\n",
        "    plt.ylabel('AUC Medio en Validaci√≥n Cruzada', fontsize=12)\n",
        "    plt.grid(True, alpha=0.5)\n",
        "    plt.show()\n",
        "\n",
        "# --- 7. COMPARACI√ìN GR√ÅFICA FINAL ---\n",
        "print(\"\\n--- Comparaci√≥n Final: Modelo Default vs. Modelo Optimizado ---\")\n",
        "optimized_rf_model = best_search.best_estimator_\n",
        "\n",
        "# Evaluaci√≥n en conjunto de prueba\n",
        "comparison_results = []\n",
        "\n",
        "# Modelo base\n",
        "comparison_results.append({\n",
        "    'Modelo': 'Base',\n",
        "    'Accuracy': base_metrics['Accuracy'],\n",
        "    'Precision': base_metrics['Precision'],\n",
        "    'Recall': base_metrics['Recall'],\n",
        "    'F1 Score': base_metrics['F1 Score'],\n",
        "    'AUC': base_metrics['AUC']\n",
        "})\n",
        "\n",
        "# Modelo optimizado\n",
        "y_pred_optimized = optimized_rf_model.predict(X_test_scaled)\n",
        "y_prob_optimized = optimized_rf_model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "comparison_results.append({\n",
        "    'Modelo': 'Optimizado',\n",
        "    'Accuracy': accuracy_score(y_test, y_pred_optimized),\n",
        "    'Precision': precision_score(y_test, y_pred_optimized),\n",
        "    'Recall': recall_score(y_test, y_pred_optimized),\n",
        "    'F1 Score': f1_score(y_test, y_pred_optimized),\n",
        "    'AUC': roc_auc_score(y_test, y_prob_optimized)\n",
        "})\n",
        "\n",
        "df_comparison = pd.DataFrame(comparison_results)\n",
        "print(\"\\nComparaci√≥n detallada:\")\n",
        "print(df_comparison.round(4))\n",
        "\n",
        "# Gr√°fico de comparaci√≥n\n",
        "df_comparison_long = df_comparison.melt(id_vars='Modelo', var_name='M√©trica', value_name='Puntuaci√≥n')\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "ax = sns.barplot(data=df_comparison_long, x='M√©trica', y='Puntuaci√≥n', hue='Modelo', palette='cividis')\n",
        "\n",
        "for container in ax.containers:\n",
        "    ax.bar_label(container, fmt='%.3f', fontsize=11, padding=3)\n",
        "\n",
        "plt.title('Comparaci√≥n de Rendimiento: Modelo Base vs. Modelo Optimizado', fontsize=18)\n",
        "plt.xlabel('M√©trica de Evaluaci√≥n', fontsize=14)\n",
        "plt.ylabel('Puntuaci√≥n', fontsize=14)\n",
        "plt.ylim(0, 1.0)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "plt.legend(title='Versi√≥n del Modelo')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- 8. RECOMENDACIONES FINALES ---\n",
        "print(f\"\\n--- Recomendaciones y Mejores Par√°metros ---\")\n",
        "print(f\"Mejores hiperpar√°metros encontrados:\")\n",
        "for param, value in best_search.best_params_.items():\n",
        "    print(f\"  {param}: {value}\")\n",
        "\n",
        "if best_auc > target_auc:\n",
        "    print(f\"\\n‚úÖ El modelo optimizado muestra mejora significativa.\")\n",
        "    print(f\"   Guardando modelo optimizado...\")\n",
        "    # joblib.dump(optimized_rf_model, 'modelo_optimizado.pkl')\n",
        "else:\n",
        "    print(f\"\\n‚ùå Considera las siguientes acciones:\")\n",
        "    print(\"   - Revisar ingenier√≠a de caracter√≠sticas\")\n",
        "    print(\"   - Aumentar el conjunto de datos\")\n",
        "    print(\"   - Probar algoritmos diferentes\")\n",
        "    print(\"   - Ajustar la estrategia de validaci√≥n cruzada\")"
      ],
      "metadata": {
        "id": "0oEszEVsiUsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1s_y9qyYiex2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}